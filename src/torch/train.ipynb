{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#create a train / test set from train.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "df = pd.read_csv(\"../../data/train.csv\")\n",
    "\n",
    "\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df[mask].to_csv(\"../../data/train_val_split/train.csv\", index=False)\n",
    "df[~mask].to_csv(\"../../data/train_val_split/val.csv\", index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 217,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "2  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "3  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "4  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00025465d4725e87</td>\n      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "source": [
    "pd.read_csv(\"../../data/train_val_split/train.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from model.model import LSTM\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "TEXT = Field(tokenize = 'basic_english', lower=True, sequential=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "fields = [\n",
    "    ('id',None),  #ignore id col\n",
    "    ('comment_text', TEXT), \n",
    "    ('toxic', LABEL),\n",
    "    ('severe_toxic', LABEL),\n",
    "    ('obscene', LABEL),\n",
    "    ('threat', LABEL),\n",
    "    ('insult', LABEL),\n",
    "    ('identity_hate', LABEL)\n",
    "]\n",
    "\n",
    "train_ds, val_ds = TabularDataset.splits(path=\"../../data/train_val_split\",train=\"train.csv\", validation=\"val.csv\",format=\"csv\", skip_header=True, fields=fields)\n",
    "\n",
    "# print(vars(train_ds[15]))\n",
    "\n",
    "TEXT.build_vocab(train_ds, vectors=\"fasttext.simple.300d\")\n",
    "LABEL.build_vocab(train_ds)\n",
    "\n",
    "# create iterators for train/valid/test datasets\n",
    "train_iter, val_iter = BucketIterator.splits(\n",
    " (train_ds, val_ds), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(64, 64),\n",
    " device=device, # if you want to use the GPU, specify the GPU number here\n",
    " sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    " sort_within_batch=False,\n",
    " repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.comment_text]:[torch.LongTensor of size 461x64]\n",
       "\t[.toxic]:[torch.LongTensor of size 64]\n",
       "\t[.severe_toxic]:[torch.LongTensor of size 64]\n",
       "\t[.obscene]:[torch.LongTensor of size 64]\n",
       "\t[.threat]:[torch.LongTensor of size 64]\n",
       "\t[.insult]:[torch.LongTensor of size 64]\n",
       "\t[.identity_hate]:[torch.LongTensor of size 64]"
      ]
     },
     "metadata": {},
     "execution_count": 220
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x \n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "\n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "        yield (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "valid_dl = BatchWrapper(val_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 213, 3615,  269,  ...,   10,   26,    6],\n",
       "         [ 443,    5,    4,  ...,  257,   10,    9],\n",
       "         [ 384, 1372,   36,  ...,   23,  702,  169],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " tensor([[1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 224
    }
   ],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(209350, 300)\n",
       "  (encoder): LSTM(300, 500)\n",
       "  (linear1): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (linear2): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (fc): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 237
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_dim=300,num_classes=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=1\n",
    "                          )\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        out = self.linear1(feature)\n",
    "        out = self.linear2(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = LSTM(hidden_dim=500, embedding_dim=300)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimpleLSTMBaseline(\n",
       "  (embedding): Embedding(209350, 100)\n",
       "  (encoder): LSTM(100, 500)\n",
       "  (linear_layers): ModuleList(\n",
       "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
       "  )\n",
       "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 235
    }
   ],
   "source": [
    "class SimpleLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "          feature = layer(feature)\n",
    "          preds = self.predictor(feature)\n",
    "        return preds\n",
    "\n",
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleLSTMBaseline(nh, emb_dim=em_sz, num_linear=nl)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(209350, 300)\n",
       "  (encoder): LSTM(300, 500)\n",
       "  (linear1): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (linear2): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (fc): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/1994 [00:34<18:49:26, 34.00s/it]\n",
      "  0%|          | 0/1994 [00:00<?, ?it/s]Epoch: 1, Training Loss: 0.3321, Validation Loss: 7.6338\n",
      "  0%|          | 1/1994 [00:34<19:11:30, 34.67s/it]\n",
      "Epoch: 2, Training Loss: 0.2912, Validation Loss: 7.6338\n"
     ]
    }
   ],
   "source": [
    "def train(train_dl):\n",
    "    loss = 0\n",
    "    for x, y in train_dl:\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        loss.backward()\n",
    "        loss.step()\n",
    "        loss +=loss.item()\n",
    "    return lss\n",
    "def val(val_dl):\n",
    "    loss = 0\n",
    "    for x, y in val_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        loss +=loss.item()\n",
    "    return lss\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm.tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_iter)\n",
    "\n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(val_iter)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}